{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据分析实战"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 04 Python科学计算：用NumPy快速处理数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "x=2\n",
    "x*=2\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3,)\n",
      "(3, 3)\n",
      "int64\n",
      "[[1 2 3]\n",
      " [4 5 6]\n",
      " [7 8 9]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a=np.array([1,2,3])\n",
    "b=np.array([[1,2,3],[4,5,6],[7,8,9]])\n",
    "print(a.shape)\n",
    "print(b.shape)\n",
    "print(a.dtype)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 自定义类型\n",
    "person_type = np.dtype({\n",
    "    'names': ['name', 'age', 'chinese', 'math', 'english'],\n",
    "    'formats': ['S32', 'i', 'i', 'i', 'f']})\n",
    "\n",
    "peoples = np.array([\n",
    "    (\"ZhangFei\", 32, 75, 100, 90),\n",
    "    (\"GuanYu\", 24, 85, 96, 88.5),\n",
    "    (\"ZhaoYun\", 28, 85, 92, 96.5),\n",
    "    (\"HuangZhong\", 29, 65, 85, 100)], dtype=person_type)\n",
    "\n",
    "ages = peoples[:]['age']\n",
    "chinese = peoples[:]['chinese']\n",
    "math = peoples[:]['math']\n",
    "english = peoples[:]['english']\n",
    "\n",
    "# 计算平均值\n",
    "print(np.mean(ages))\n",
    "print(np.mean(chinese))\n",
    "print(np.mean(math))\n",
    "print(np.mean(english))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 初始值、终值和步长\n",
    "x1=np.arange(1,11,2)\n",
    "\n",
    "#  初始值、终值和元素个数\n",
    "x2=np.linspace(1,9,5)\n",
    "\n",
    "print(x1)\n",
    "print(x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 算术运算\n",
    "print(np.add(x1,x2))\n",
    "print(np.subtract(x1,x2))\n",
    "print(np.multiply(x1,x2))\n",
    "print(np.divide(x1,x2))\n",
    "print(np.power(x1,x2))\n",
    "print(np.remainder(x1,x2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 统计运算\n",
    "import numpy as np\n",
    "a = np.array([[1,2,3],[4,5,6],[7,8,9]])\n",
    "\n",
    "# 统计矩阵最小值\n",
    "print(np.amin(a))\n",
    "print(np.amin(a,0))\n",
    "print(np.amin(a,1))\n",
    "\n",
    "# 统计矩阵最大值\n",
    "print(np.amax(a))\n",
    "print(np.amax(a,0))\n",
    "print(np.amax(a,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 统计最大值和最小值之差\n",
    "print(np.ptp(a))\n",
    "print(np.ptp(a,0))\n",
    "print(np.ptp(a,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 统计数组的百分位数 percentile()\n",
    "print(np.percentile(a,50))\n",
    "print(np.percentile(a,50,axis=0))\n",
    "print(np.percentile(a,50,axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 统计数组中的中位数 median()\n",
    "print(np.median(a))\n",
    "print(np.median(a,axis=0))\n",
    "print(np.median(a,axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 统计数组中的平均数 mean()\n",
    "print(np.mean(a))\n",
    "print(np.mean(a,axis=0))\n",
    "print(np.mean(a,axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加权平均值 average()\n",
    "source=np.array([1,2,3,4])\n",
    "wts=np.array([1,2,3,4])\n",
    "print(np.average(a))\n",
    "# 加权\n",
    "print(np.average(source,weights=wts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 统计数组中的标准差 std() 、方差 var()\n",
    "source = np.array([1, 2, 3, 4])\n",
    "print(np.std(source))\n",
    "print(np.var(source))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numpy 排序\n",
    "source=np.array([[4,2,3,1],[2,4,1,7],[3,2,6,0]])\n",
    "print(np.sort(source))\n",
    "print(np.sort(source,axis=None))\n",
    "print(np.sort(source,axis=0))\n",
    "print(np.sort(source,axis=0))\n",
    "print(np.sort(source,axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 实例\n",
    "import numpy as np\n",
    "\n",
    "def show(name,data):\n",
    "    print('%s，平均成绩：%d,最小成绩：%d,最大成绩：%d,方差：%d,标准差：%d' % (name,np.average(data),np.amin(data),np.amax(data),np.var(data),np.std(data)))\n",
    "\n",
    "score_type=np.dtype({\n",
    "    'names':['name','chinese','english','math'],\n",
    "    'formats':['S32','i','i','i']\n",
    "})\n",
    "\n",
    "students=np.array([\n",
    "    ('zhangfei',66,65,30),\n",
    "    ('guanyu',95,85,98),\n",
    "    ('zhaoyun',93,92,96),\n",
    "    ('huangzhong',90,88,77),\n",
    "    ('dianwei',80,90,90)\n",
    "],dtype=score_type)\n",
    "\n",
    "show('chinese',students[:]['chinese'])\n",
    "show('english',students[:]['english'])\n",
    "show('math',students[:]['math'])\n",
    "\n",
    "rank=sorted(students,key=lambda x:x[1]+x[2]+x[3],reverse=True)\n",
    "print(rank)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 05 Python科学计算：Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import Series,DataFrame\n",
    "\n",
    "x1=Series([1,2,3,4,5,6,7,8,9])\n",
    "x2=Series(data=[1,2,3,4,5],index=['a','b','c','d','e'])\n",
    "\n",
    "data={'a':1,'b':2,'c':3,'d':4,'e':5}\n",
    "x3=Series(data)\n",
    "\n",
    "print(x1)\n",
    "print(x2)\n",
    "print(x3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import Series, DataFrame\n",
    "import numpy as np\n",
    "\n",
    "data = {'Chinese': [66, 95, 93, 90,80],'English': [65, 85, 92, 88, 90],'Math': [30, 98, 96, 77, 90]}\n",
    "df1= DataFrame(data)\n",
    "df2 = DataFrame(data, index=['ZhangFei', 'GuanYu', 'ZhaoYun', 'HuangZhong', 'DianWei'], columns=['English', 'Math', 'Chinese'])\n",
    "\n",
    "print(df1)\n",
    "print(df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 清洗数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 删除 DataFrame 中的不必要的列和行\n",
    "del_columns=df2.drop(columns=['Chinese'])\n",
    "print(del_columns)\n",
    "\n",
    "del_line=df2.drop(index=['ZhangFei'])\n",
    "print(del_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 重命名列名 columns\n",
    "df2.rename(columns={'Chinese':'YunWen','English':'YinYu'},inplace=True)\n",
    "print(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 去除重复行\n",
    "remove_duplicate=df2.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['YunWen'].astype(np.int64)\n",
    "df2['YunWen']= df2['YunWen'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 删除左右两边空格\n",
    "df2['YunWen']=df2['YunWen'].map(str.strip)\n",
    "# 删除左边空格\n",
    "df2['YunWen']=df2['YunWen'].map(str.lstrip)\n",
    "# 删除右边空格\n",
    "df2['YunWen']=df2['YunWen'].map(str.rstrip)\n",
    "\n",
    "# 去除特殊字符\n",
    "df2['YunWen']=df2['YunWen'].str.strip('$')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 全部大写\n",
    "df2.columns=df2.columns.str.upper()\n",
    "# 全部小写\n",
    "df2.columns=df2.columns.str.lower()\n",
    "# 首字母大写\n",
    "df2.columns=df2.columns.str.title()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 统计函数合集\n",
    "df3 = DataFrame({'name':['ZhangFei', 'GuanYu', 'a', 'b', 'c'], 'data1':range(5)})\n",
    "print(df3.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据表合并\n",
    "df4 = DataFrame({'name':['ZhangFei', 'GuanYu', 'a', 'b', 'c'], 'data1':range(5)})\n",
    "df5 = DataFrame({'name':['ZhangFei', 'GuanYu', 'A', 'B', 'C'], 'data2':range(5)})\n",
    "\n",
    "# 内连接（交集）\n",
    "df6 = pd.merge(df4,df5,how='inner')\n",
    "# 左连接\n",
    "df6 = pd.merge(df4,df5,how='left')\n",
    "# 右连接\n",
    "df6 = pd.merge(df4,df5,how='right')\n",
    "# 外连接（并集）\n",
    "df6 = pd.merge(df4,df5,how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用 SQL\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "from pandasql import sqldf, load_meat, load_births\n",
    "df7 = DataFrame({'name':['ZhangFei', 'GuanYu', 'a', 'b', 'c'], 'data1':range(5)})\n",
    "pysqldf = lambda sql: sqldf(sql, globals())\n",
    "sql = \"select * from df7 where name ='ZhangFei'\"\n",
    "print(pysqldf(sql))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 例子\n",
    "import pandas as pd\n",
    "from pandas import Series, DataFrame\n",
    "import numpy as np\n",
    "\n",
    "data = {'Chinese': [66, 95, 95, 90,80,80],'English': [65, 85, 92, 88, 90,90],'Math': [None, 98, 96, 77, 90,90]}\n",
    "df = DataFrame(data, index=['ZhangFei', 'GuanYu', 'ZhaoYun', 'HuangZhong', 'DianWei','DianWei'], columns=['Chinese','English', 'Math' ])\n",
    "\n",
    "print(df)\n",
    "\n",
    "df=df.drop_duplicates()\n",
    "df['Total']=df.sum(axis=1)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13 数据变换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Min-Max 规范化\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "\n",
    "x = np.array([\n",
    "    [0.,-3.,1.],\n",
    "    [3.,1.,2.],\n",
    "    [0.,1.,-1.]\n",
    "])\n",
    "\n",
    "min_max_scaler=preprocessing.MinMaxScaler()\n",
    "minmax_x=min_max_scaler.fit_transform(x)\n",
    "\n",
    "print(minmax_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Z-Score 规范化\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "\n",
    "x = np.array([\n",
    "    [0.,-3.,1.],\n",
    "    [3.,1.,2.],\n",
    "    [0.,1.,-1.]\n",
    "])\n",
    "\n",
    "scaled_x=preprocessing.scale(x)\n",
    "print(scaled_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. 小数定标规范化\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "\n",
    "x = np.array([\n",
    "    [0.,-3.,1.],\n",
    "    [3.,1.,2.],\n",
    "    [0.,1.,-1.]\n",
    "])\n",
    "\n",
    "j=np.ceil(np.log10(np.max(abs(x))))\n",
    "scaled_x=x/(10**j)\n",
    "print(scaled_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 习题\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "\n",
    "x = np.array([\n",
    "    [5000.],\n",
    "    [16000.],\n",
    "    [58000.]\n",
    "])\n",
    "\n",
    "min_max_scaler=preprocessing.MinMaxScaler()\n",
    "minmax_x=min_max_scaler.fit_transform(x)\n",
    "\n",
    "print(minmax_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15 数据可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 散点图\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "N=1000\n",
    "x=np.random.rand(N)\n",
    "y=np.random.rand(N)\n",
    "\n",
    "plt.scatter(x,y,marker='x')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'x':x,'y':y})\n",
    "sns.jointplot(x='x',y='y',data=df,kind='scatter')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 折线图\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "x = [2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019]\n",
    "y = [5, 3, 6, 20, 17, 16, 19, 30, 32, 35]\n",
    "\n",
    "plt.plot(x,y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame({'x':x,'y':y})\n",
    "sns.lineplot(x='x',y='y',data=df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 直方图\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "a=np.random.rand(100)\n",
    "s=pd.Series(a)\n",
    "\n",
    "plt.hist(s)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(s,kde=False)\n",
    "plt.show()\n",
    "sns.distplot(s,kde=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 条形图\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "x=['Cat1','Cat2','Cat3','Cat4','Cat5']\n",
    "y=[5,4,8,12,7]\n",
    "\n",
    "plt.bar(x,y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x,y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 箱线图\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "data=np.random.normal(size=(10,4))\n",
    "labels=['A','B','C','D']\n",
    "\n",
    "plt.boxplot(data,labels=labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame(data,columns=labels)\n",
    "sns.boxplot(data=df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 饼图\n",
    "import matplotlib.pyplot as plt\n",
    "# 数据准备\n",
    "nums = [25, 37, 33, 37, 6]\n",
    "labels = ['High-school','Bachelor','Master','Ph.d', 'Others']\n",
    "# 用Matplotlib画饼图\n",
    "plt.pie(x = nums, labels=labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 热力图\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "# 数据准备\n",
    "flights = DataFrame(pd.read_csv('~/Project/seaborn-data/flights.csv'))\n",
    "data=flights.pivot('year','month','passengers')\n",
    "# 用Seaborn画热力图\n",
    "sns.heatmap(data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 蜘蛛图\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.font_manager import FontProperties\n",
    "\n",
    "labels=np.array([u\"推进\",\"KDA\",u\"生存\",u\"团战\",u\"发育\",u\"输出\"])\n",
    "stats=[83, 61, 95, 67, 76, 88]\n",
    "\n",
    "angles=np.linspace(0,2*np.pi,len(labels),endpoint=False)\n",
    "stats=np.concatenate((stats,[stats[0]]))\n",
    "angles=np.concatenate((angles,[angles[0]]))\n",
    "\n",
    "fig=plt.figure()\n",
    "ax=fig.add_subplot(111,polar=True)\n",
    "ax.plot(angles, stats, 'o-', linewidth=2)\n",
    "ax.fill(angles, stats, alpha=0.25)\n",
    "# 设置中文字体\n",
    "font = FontProperties(fname=r\"C:\\Windows\\Fonts\\simhei.ttf\", size=14)  \n",
    "ax.set_thetagrids(angles * 180/np.pi, labels, FontProperties=font)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File /Users/mason/Project/seaborn-data/tips.csv does not exist: '/Users/mason/Project/seaborn-data/tips.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-b831935d5bad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# 数据准备\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mtips\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'~/Project/seaborn-data/tips.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;31m# print(tips.head(10))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# 用Seaborn画二元变量分布图（散点图，核密度图，Hexbin图）\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    674\u001b[0m         )\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    878\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1112\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1114\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1115\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1889\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1891\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1892\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1893\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File /Users/mason/Project/seaborn-data/tips.csv does not exist: '/Users/mason/Project/seaborn-data/tips.csv'"
     ]
    }
   ],
   "source": [
    "# 二元变量分布\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "# 数据准备\n",
    "tips = DataFrame(pd.read_csv('~/Project/seaborn-data/tips.csv'))\n",
    "# print(tips.head(10))\n",
    "# 用Seaborn画二元变量分布图（散点图，核密度图，Hexbin图）\n",
    "sns.jointplot(x=\"total_bill\", y=\"tip\", data=tips, kind='scatter')\n",
    "sns.jointplot(x=\"total_bill\", y=\"tip\", data=tips, kind='kde')\n",
    "sns.jointplot(x=\"total_bill\", y=\"tip\", data=tips, kind='hex')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 成对关系\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "# 数据准备\n",
    "iris = DataFrame(pd.read_csv('~/Project/seaborn-data/iris.csv'))\n",
    "# 用Seaborn画成对关系\n",
    "sns.pairplot(iris)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 决策树（上）：要不要去打篮球？决策树来告诉你"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.datasets import load_iris\n",
    "from pandas import DataFrame\n",
    "import graphviz\n",
    "\n",
    "# 准备数据集\n",
    "iris = load_iris()\n",
    "# 获取特征集和分类标识\n",
    "features = iris.data\n",
    "labels = iris.target\n",
    "\n",
    "# 随机抽取 33% 的数据作为测试集，其余为训练集\n",
    "train_features,test_features,train_labels,test_labels = train_test_split(features,labels,test_size=0.33,random_state=0)\n",
    "\n",
    "# 创建 CART 分类树\n",
    "clf = DecisionTreeClassifier(criterion='gini')\n",
    "# 拟合构造 CART 分类树\n",
    "clf = clf.fit(train_features,train_labels)\n",
    "# 用 CART 分类树做预测\n",
    "test_predict = clf.predict(test_features)\n",
    "score = accuracy_score(test_labels,test_predict)\n",
    "print(\"CART 分类树准确率 %.4lf\" % score)\n",
    "# 决策树\n",
    "# dot_data = export_graphviz(clf,out_file=None)\n",
    "# graph = graphviz.Source(dot_data)\n",
    "# graph.render('iris')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 决策树（中）：CART，一棵是回归树，另一棵是分类树"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.metrics import r2_score,mean_absolute_error,mean_squared_error\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# 准备数据集\n",
    "boston = load_boston()\n",
    "# 探索数据\n",
    "print(boston.feature_names)\n",
    "# 获取特征集和房价\n",
    "features = boston.data\n",
    "prices = boston.target\n",
    "# 随机抽取33%的数据作为测试集，其余为训练集\n",
    "train_features,test_features,train_price,test_price = train_test_split(features,prices,test_size=0.33)\n",
    "# 创建CART回归树\n",
    "dtr = DecisionTreeRegressor()\n",
    "# 拟合构造CART回归树\n",
    "dtr.fit(train_features,train_price)\n",
    "# 预测测试集中的房价\n",
    "predict_price = dtr.predict(test_features)\n",
    "# 测试集的结果评价\n",
    "print('回归树二乘偏差均值',mean_squared_error(test_price,predict_price))\n",
    "print('回归树绝对值偏差均值',mean_absolute_error(test_price,predict_price))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 决策树（下）：泰坦尼克乘客生存预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# 数据加载\n",
    "train_data = pd.read_csv('~/Project/Titanic_Data/train.csv')\n",
    "test_data = pd.read_csv('~/Project/Titanic_Data/test.csv')\n",
    "# print(train_data.info())\n",
    "# print(train_data.describe())\n",
    "\n",
    "# 使用平均年龄来填充年龄中的 nan 值\n",
    "train_data['Age'].fillna(train_data['Age'].mean(),inplace=True)\n",
    "test_data['Age'].fillna(test_data['Age'].mean(),inplace=True)\n",
    "# 使用票价的均值填充票价中的 nan 值\n",
    "train_data['Fare'].fillna(train_data['Fare'].mean(),inplace=True)\n",
    "test_data['Fare'].fillna(test_data['Fare'].mean(),inplace=True)\n",
    "\n",
    "# print(train_data['Embarked'].value_counts())\n",
    "\n",
    "# 使用登陆最多的港口来填充登陆港口的 nan 值\n",
    "train_data['Embarked'].fillna('S',inplace=True)\n",
    "test_data['Embarked'].fillna('S',inplace=True)\n",
    "\n",
    "# 特征选择\n",
    "features = ['Pclass','Sex','Age','SibSp','Parch','Fare','Embarked']\n",
    "train_features = train_data[features]\n",
    "train_labels = train_data['Survived']\n",
    "test_features = test_data[features]\n",
    "\n",
    "dvec = DictVectorizer(sparse=False)\n",
    "train_features = dvec.fit_transform(train_features.to_dict(orient='record'))\n",
    "print(dvec.feature_names_)\n",
    "\n",
    "# 决策树模型\n",
    "# 构造 ID3 决策树\n",
    "clf = DecisionTreeClassifier(criterion='entropy')\n",
    "# 决策树训练\n",
    "clf.fit(train_features,train_labels)\n",
    "\n",
    "test_features = dvec.transform(test_features.to_dict(orient='record'))\n",
    "# 决策树预测\n",
    "pred_labels=clf.predict(test_features)\n",
    "\n",
    "# 得到决策树的准确率\n",
    "acc_decision_tree = round(clf.score(train_features,train_labels),6)\n",
    "print(u'score 准确率 %.4lf' % acc_decision_tree)\n",
    "\n",
    "# 使用 k 折交叉验证 统计决策树准确率\n",
    "print(u'cross_val_score 准确率 %.4lf' % np.mean(cross_val_score(clf,train_features,train_labels,cv=10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 朴素贝叶斯分类（下）：如何对文档进行分类？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vec = TfidfVectorizer()\n",
    "\n",
    "documents = [\n",
    "    'this is the bayes document',\n",
    "    'this is the second second document',\n",
    "    'and the third one',\n",
    "    'is this the document'\n",
    "]\n",
    "tfidf_matrix = tfidf_vec.fit_transform(documents)\n",
    "\n",
    "print('不重复的词:',tfidf_vec.get_feature_names())\n",
    "print('每个单词的ID:',tfidf_vec.vocabulary_)\n",
    "print('每个单词的 tfidf 值:',tfidf_matrix.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "import os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "#  加载停用词列表\n",
    "def load_stop_wrods():\n",
    "    stop_word=''\n",
    "    with open('text_classification/text classification/stop/stopword.txt','r', encoding='utf-8') as f:\n",
    "        stop_word=f.read();\n",
    "        stop_word= stop_word.encode('utf-8').decode('utf-8-sig')\n",
    "        stop_word=stop_word.split('\\n')\n",
    "    return stop_word\n",
    "\n",
    "# 加载文件\n",
    "def load_document(file_dir,label):\n",
    "    file_list = os.listdir(file_dir)\n",
    "    words_list = []\n",
    "    label_list = []\n",
    "    for file in file_list:\n",
    "        file_path=os.path.join(file_dir,file)\n",
    "        words_list.append(cut_text(file_path))\n",
    "        label_list.append(label)\n",
    "    return words_list,label_list\n",
    "\n",
    "# 分词\n",
    "def cut_text(file_path):\n",
    "    word_spilit=''\n",
    "    result = ''\n",
    "    with open(file_path,'r', encoding='gb18030') as f:\n",
    "        text=f.read()\n",
    "        cut_text = jieba.cut(text)\n",
    "        result=word_spilit.join(cut_text)\n",
    "    return result\n",
    "\n",
    "\n",
    "# 训练数据\n",
    "train_words_list1, train_labels1 = load_document('text_classification/text classification/train/女性', '女性')\n",
    "train_words_list2, train_labels2 = load_document('text_classification/text classification/train/体育', '体育')\n",
    "train_words_list3, train_labels3 = load_document('text_classification/text classification/train/文学', '文学')\n",
    "train_words_list4, train_labels4 = load_document('text_classification/text classification/train/校园', '校园')\n",
    "\n",
    "train_words_list = train_words_list1 + train_words_list2 + train_words_list3 + train_words_list4\n",
    "train_labels = train_labels1 + train_labels2 + train_labels3 + train_labels4\n",
    "\n",
    "# 测试数据\n",
    "test_words_list1, test_labels1 = load_document('text_classification/text classification/test/女性', '女性')\n",
    "test_words_list2, test_labels2 = load_document('text_classification/text classification/test/体育', '体育')\n",
    "test_words_list3, test_labels3 = load_document('text_classification/text classification/test/文学', '文学')\n",
    "test_words_list4, test_labels4 = load_document('text_classification/text classification/test/校园', '校园')\n",
    "\n",
    "test_words_list = test_words_list1 + test_words_list2 + test_words_list3 + test_words_list4\n",
    "test_labels = test_labels1 + test_labels2 + test_labels3 + test_labels4\n",
    "\n",
    "stop_words=load_stop_wrods()\n",
    "\n",
    "# 计算权重\n",
    "train_tf=TfidfVectorizer(stop_words=stop_words,max_df=0.5)\n",
    "train_features=train_tf.fit_transform(train_words_list)\n",
    "\n",
    "# 多项式贝叶斯分类器\n",
    "clf=MultinomialNB(alpha=0.01).fit(train_features,train_labels)\n",
    "\n",
    "# 使用生成分类器做预测\n",
    "test_tf=TfidfVectorizer(stop_words=stop_words,max_df=0.5,vocabulary=tf.vocabulary_)\n",
    "test_features=test_tf.fit_transform(test_words_list)\n",
    "\n",
    "predicted_labels=clf.predict(test_features)\n",
    "\n",
    "# 计算准确率\n",
    "print('准确率为：', metrics.accuracy_score(test_labels, predicted_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM（下）：如何进行乳腺癌检测？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-19T03:14:51.977579Z",
     "start_time": "2020-05-19T03:14:51.947224Z"
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File breast_cancer_data/data.csv does not exist: 'breast_cancer_data/data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-99-13f3171e60bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# 加载数据\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'breast_cancer_data/data.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_option\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'display.max_columns'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# print(data.columns)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    674\u001b[0m         )\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    878\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1112\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1114\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1115\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1889\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1891\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1892\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1893\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File breast_cancer_data/data.csv does not exist: 'breast_cancer_data/data.csv'"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import metrics\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 加载数据\n",
    "data=pd.read_csv('breast_cancer_data/data.csv')\n",
    "pd.set_option('display.max_columns',None)\n",
    "# print(data.columns)\n",
    "# print(data.head(5))\n",
    "# print(data.describe())\n",
    "\n",
    "# 将特征字段分为3组\n",
    "features_mean=list(data.columns[2:12])\n",
    "features_se=list(data.columns[12:22])\n",
    "features_worst=list(data.columns[22:32])\n",
    "\n",
    "# 删除没有用的列\n",
    "data.drop('id',axis=1,inplace=True)\n",
    "# 将B良性替换为0，M恶性替换为1\n",
    "data['diagnosis']=data['diagnosis'].map({'M':1,'B':0})\n",
    "# 将肿瘤诊断结果可视化\n",
    "sns.countplot(data['diagnosis'],label='Count')\n",
    "plt.show()\n",
    "\n",
    "# 用热力图呈现 features_mean 字段之间的相关性\n",
    "corr=data[features_mean].corr()\n",
    "plt.figure(figsize=(15,15))\n",
    "sns.heatmap(corr,annot=True)\n",
    "plt.show()\n",
    "\n",
    "# 颜色越浅代表相关性越大\n",
    "\n",
    "# 特征选择\n",
    "features_remain = ['radius_mean','texture_mean', 'smoothness_mean','compactness_mean','symmetry_mean', 'fractal_dimension_mean'] \n",
    "# 抽取 30% 的数据作为测试集，其余作为训练集\n",
    "train,test=train_test_split(data,test_size=0.3)\n",
    "train_x=train[features_remain]\n",
    "train_y=train['diagnosis']\n",
    "test_x=test[features_remain]\n",
    "test_y=test['diagnosis']\n",
    "\n",
    "ss=StandardScaler()\n",
    "train_x=ss.fit_transform(train_x)\n",
    "test_x=ss.transform(test_x)\n",
    "\n",
    "# 创建 SVM 分类器\n",
    "# model=svm.SVC()\n",
    "model=svm.LinearSVC()\n",
    "# 用训练集做训练\n",
    "model.fit(train_x,train_y)\n",
    "# 用测试集做预测\n",
    "prediction=model.predict(test_x)\n",
    "print('准确率：',metrics.accuracy_score(prediction,test_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN（下）：如何对手写数字进行识别？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-19T03:14:43.872603Z",
     "start_time": "2020-05-19T03:14:43.518504Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "findfont: Font family ['sans-serif'] not found. Falling back to DejaVu Sans.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1797, 64)\n",
      "[[ 0.  0.  5. 13.  9.  1.  0.  0.]\n",
      " [ 0.  0. 13. 15. 10. 15.  5.  0.]\n",
      " [ 0.  3. 15.  2.  0. 11.  8.  0.]\n",
      " [ 0.  4. 12.  0.  0.  8.  8.  0.]\n",
      " [ 0.  5.  8.  0.  0.  9.  8.  0.]\n",
      " [ 0.  4. 11.  0.  1. 12.  7.  0.]\n",
      " [ 0.  2. 14.  5. 10. 12.  0.  0.]\n",
      " [ 0.  0.  6. 13. 10.  0.  0.  0.]]\n",
      "0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# KNN 分类器\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "# KNN 回归\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "digits = load_digits()\n",
    "data = digits.data\n",
    "\n",
    "print(data.shape)\n",
    "print(digits.images[0])\n",
    "print(digits.target[0])\n",
    "\n",
    "plt.gray()\n",
    "plt.imshow(digits.images[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-13T11:35:19.840164Z",
     "start_time": "2020-05-13T11:35:19.831487Z"
    }
   },
   "outputs": [],
   "source": [
    "train_x,test_x,train_y,test_y = train_test_split(data,digits.target,test_size=0.25,random_state=33)\n",
    "# 采用 Z-Score 规范\n",
    "ss=preprocessing.StandardScaler()\n",
    "train_ss_x = ss.fit_transform(train_x)\n",
    "test_ss_x = ss.transform(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-13T11:39:49.971461Z",
     "start_time": "2020-05-13T11:39:49.876661Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN 准确率: 0.9756\n"
     ]
    }
   ],
   "source": [
    "# 创建 KNN 分类器\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit(train_ss_x, train_y)\n",
    "predict_y = knn.predict(test_ss_x)\n",
    "print(\"KNN 准确率: %.4lf\"%accuracy_score(test_y,predict_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-13T11:42:30.117107Z",
     "start_time": "2020-05-13T11:42:29.965061Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM 准确率: 0.9867\n"
     ]
    }
   ],
   "source": [
    "# 创建 SVM 分类器\n",
    "svm = SVC()\n",
    "svm.fit(train_ss_x,train_y)\n",
    "predict_y=svm.predict(test_ss_x)\n",
    "print('SVM 准确率: %0.4lf'%accuracy_score(test_y,predict_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-13T11:43:02.974217Z",
     "start_time": "2020-05-13T11:43:02.945141Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "多项式朴素贝叶斯准确率: 0.8844\n",
      "CART决策树准确率: 0.8644\n"
     ]
    }
   ],
   "source": [
    "# 采用Min-Max规范化\n",
    "mm = preprocessing.MinMaxScaler()\n",
    "train_mm_x = mm.fit_transform(train_x)\n",
    "test_mm_x = mm.transform(test_x)\n",
    "# 创建Naive Bayes分类器\n",
    "mnb = MultinomialNB()\n",
    "mnb.fit(train_mm_x, train_y)\n",
    "predict_y = mnb.predict(test_mm_x)\n",
    "print(\"多项式朴素贝叶斯准确率: %.4lf\" % accuracy_score(test_y, predict_y))\n",
    "# 创建CART决策树分类器\n",
    "dtc = DecisionTreeClassifier()\n",
    "dtc.fit(train_mm_x, train_y)\n",
    "predict_y = dtc.predict(test_mm_x)\n",
    "print(\"CART决策树准确率: %.4lf\" % accuracy_score(test_y, predict_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-13T12:21:15.099194Z",
     "start_time": "2020-05-13T12:21:15.091931Z"
    }
   },
   "source": [
    "## K-Means（上）：如何给20支亚洲球队做聚类？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T02:42:51.825772Z",
     "start_time": "2020-05-14T02:42:51.725429Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        国家  2019年国际排名  2018世界杯  2015亚洲杯  具类\n",
      "0       中国         73       40        7   2\n",
      "1       日本         60       15        5   1\n",
      "2       韩国         61       19        2   1\n",
      "3       伊朗         34       18        6   1\n",
      "4       沙特         67       26       10   1\n",
      "5      伊拉克         91       40        4   2\n",
      "6      卡塔尔        101       40       13   0\n",
      "7      阿联酋         81       40        6   2\n",
      "8   乌兹别克斯坦         88       40        8   2\n",
      "9       泰国        122       40       17   0\n",
      "10      越南        102       50       17   0\n",
      "11      阿曼         87       50       12   0\n",
      "12      巴林        116       50       11   0\n",
      "13      朝鲜        110       50       14   0\n",
      "14      印尼        164       50       17   0\n",
      "15      澳洲         40       30        1   1\n",
      "16     叙利亚         76       40       17   0\n",
      "17      约旦        118       50        9   0\n",
      "18     科威特        160       50       15   0\n",
      "19    巴勒斯坦         96       50       16   0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn import preprocessing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 数据源 https://github.com/cystanford/kmeans\n",
    "data = pd.read_csv('data.csv', encoding='gbk')\n",
    "train_x = data[['2019年国际排名', '2018世界杯', '2015亚洲杯']]\n",
    "kmeans = KMeans(n_clusters=3)\n",
    "\n",
    "# 规范化到[0,1]空间\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "train_x = min_max_scaler.fit_transform(train_x)\n",
    "\n",
    "# kmeans 算法\n",
    "kmeans.fit(train_x)\n",
    "predict_y = kmeans.predict(train_x)\n",
    "\n",
    "# 合并具类结果，插入到原数据中\n",
    "result = pd.concat((data, pd.DataFrame(predict_y)), axis=1)\n",
    "result.rename({0: u'具类'}, axis=1, inplace=True)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-18T06:53:06.753719Z",
     "start_time": "2020-05-18T06:53:06.469325Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn import preprocessing\n",
    "import PIL.Image as image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# 加载图像，并对数据进行规范化\n",
    "def load_image(image_path):\n",
    "    data = []\n",
    "    with open(image_path, 'rb') as f:\n",
    "        image_pixel=image.open(f)\n",
    "        width,height=image_pixel.size\n",
    "        for x in range(width):\n",
    "            for y in range(height):\n",
    "                c1,c2,c3=image_pixel.getpixel((x,y))\n",
    "                data.append([c1,c2,c3])\n",
    "    mm = preprocessing.MinMaxScaler()\n",
    "    data=mm.fit_transform(data)\n",
    "    return np.mat(data),width,height\n",
    "\n",
    "image_mat,width,height=load_image('weixin.jpg')\n",
    "kmeans=KMeans(n_clusters=2)\n",
    "label= kmeans.fit_predict(image_mat)\n",
    "label= label.reshape([width,height])\n",
    "pic_mark=image.new(\"L\",(width,height))\n",
    "for x in range(width):\n",
    "    for y in range(height):\n",
    "        pic_mark.putpixel((x,y),int(256/(label[x][y]+1))-1)\n",
    "pic_mark.save('weixin_mark.jpg','JPEG')\n",
    "weixin_mark=image.open('weixin_mark.jpg')\n",
    "weixin_mark.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](weixin.jpg)\n",
    "![](weixin_mark.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-18T07:45:06.203631Z",
     "start_time": "2020-05-18T07:45:05.888611Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: FutureWarning: The new recommended value for bg_label is 0. Until version 0.19, the default bg_label value is -1. From version 0.19, the bg_label default value will be 0. To avoid this warning, please explicitly set bg_label value.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "from skimage import color\n",
    "\n",
    "label_color=(color.label2rgb(label)*255).astype(np.uint8)\n",
    "label_color=label_color.transpose(1,0,2)\n",
    "images=image.fromarray(label_color)\n",
    "images.save('weixin_mark_color.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-18T07:30:17.663395Z",
     "start_time": "2020-05-18T07:30:01.426889Z"
    }
   },
   "source": [
    "![](weixin_mark_color.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-18T08:02:03.986614Z",
     "start_time": "2020-05-18T08:02:03.811484Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:26: FutureWarning: The new recommended value for bg_label is 0. Until version 0.19, the default bg_label value is -1. From version 0.19, the bg_label default value will be 0. To avoid this warning, please explicitly set bg_label value.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn import preprocessing\n",
    "import PIL.Image as image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from skimage import color\n",
    "\n",
    "# 加载图像，并对数据进行规范化\n",
    "def load_image(image_path):\n",
    "    data = []\n",
    "    with open(image_path, 'rb') as f:\n",
    "        image_pixel=image.open(f)\n",
    "        width,height=image_pixel.size\n",
    "        for x in range(width):\n",
    "            for y in range(height):\n",
    "                c1,c2,c3=image_pixel.getpixel((x,y))\n",
    "                data.append([c1,c2,c3])\n",
    "    mm = preprocessing.MinMaxScaler()\n",
    "    data=mm.fit_transform(data)\n",
    "    return np.mat(data),width,height\n",
    "\n",
    "image_mat,width,height=load_image('baby.jpg')\n",
    "kmeans=KMeans(n_clusters=2)\n",
    "label=kmeans.fit_predict(image_mat)\n",
    "label=label.reshape([width,height])\n",
    "label_color=(color.label2rgb(label)*255).astype(np.uint8)\n",
    "label_color=label_color.transpose(1,0,2)\n",
    "images=image.fromarray(label_color)\n",
    "images.save('baby_mark_color.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EM聚类（下）：用EM算法对王者荣耀英雄进行划分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-19T03:17:30.892065Z",
     "start_time": "2020-05-19T03:17:30.791477Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1008x1008 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11  6  9 27  4  4 17  9  2 21  8 21 27 25 22  8 18  2  4 23 19  5 23  5\n",
      "  5  5 23 24  3 15 25 15 28  3 15 15  3 10  0 12 28  3  3 15  3 28 21  0\n",
      " 21  3 29 16  4 20 20 11 25  6 26  4 11  7 14 11 13 16 16  1 24]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "data_ori=pd.read_csv('heros.csv',encoding='gb18030')\n",
    "features=[u'最大生命',u'生命成长',u'初始生命',u'最大法力', u'法力成长',u'初始法力',u'最高物攻',u'物攻成长',u'初始物攻',u'最大物防',u'物防成长',u'初始物防', u'最大每5秒回血', u'每5秒回血成长', u'初始每5秒回血', u'最大每5秒回蓝', u'每5秒回蓝成长', u'初始每5秒回蓝', u'最大攻速', u'攻击范围']\n",
    "data=data_ori[features]\n",
    "\n",
    "plt.rcParams['font.sans-serif']=['SimHei']\n",
    "plt.rcParams['axes.unicode_minus']=False\n",
    "\n",
    "corr=data[features].corr()\n",
    "plt.figure(figsize=(14,14))\n",
    "plt.show()\n",
    "\n",
    "features_remain=[u'最大生命', u'初始生命', u'最大法力', u'最高物攻', u'初始物攻', u'最大物防', u'初始物防', u'最大每5秒回血', u'最大每5秒回蓝', u'初始每5秒回蓝', u'最大攻速', u'攻击范围']\n",
    "data=data_ori[features_remain]\n",
    "data[u'最大攻速']=data[u'最大攻速'].apply(lambda x:float(x.strip('%'))/100)\n",
    "data[u'攻击范围']=data[u'攻击范围'].map({'远程':1,'近战':0})\n",
    "\n",
    "ss=StandardScaler()\n",
    "data=ss.fit_transform(data)\n",
    "\n",
    "gmm=GaussianMixture(n_components=30,covariance_type='full')\n",
    "gmm.fit(data)\n",
    "\n",
    "prediction=gmm.predict(data)\n",
    "print(prediction)\n",
    "\n",
    "data_ori.insert(0,'分组',prediction)\n",
    "data_ori.to_csv('hero_out.csv',index=False,sep=',')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "pygments_lexer": "ipython3",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "499px",
    "left": "735px",
    "right": "20px",
    "top": "83px",
    "width": "363px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
