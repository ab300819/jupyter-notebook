{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37664bit4b80bb0ff83a43bf9a155307a346427c",
   "display_name": "Python 3.7.6 64-bit"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据分析实战"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 04 Python科学计算：用NumPy快速处理数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=2\n",
    "x*=2\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "a=np.array([1,2,3])\n",
    "b=np.array([[1,2,3],[4,5,6],[7,8,9]])\n",
    "print(a.shape)\n",
    "print(b.shape)\n",
    "print(a.dtype)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 自定义类型\n",
    "person_type=np.dtype({\n",
    "    'names':['name','age','chinese','math','english'],\n",
    "    'formats':['S32','i','i','i','f']})\n",
    "\n",
    "peoples=np.array([\n",
    "    (\"ZhangFei\",32,75,100, 90),\n",
    "    (\"GuanYu\",24,85,96,88.5),       \n",
    "    (\"ZhaoYun\",28,85,92,96.5),\n",
    "    (\"HuangZhong\",29,65,85,100)],dtype=person_type)\n",
    "\n",
    "ages=peoples[:]['age']\n",
    "chinese=peoples[:]['chinese']\n",
    "math=peoples[:]['math']\n",
    "english=peoples[:]['english']\n",
    "\n",
    "# 计算平均值\n",
    "print(np.mean(ages))\n",
    "print(np.mean(chinese))\n",
    "print(np.mean(math))\n",
    "print(np.mean(english))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 初始值、终值和步长\n",
    "x1=np.arange(1,11,2)\n",
    "\n",
    "#  初始值、终值和元素个数\n",
    "x2=np.linspace(1,9,5)\n",
    "\n",
    "print(x1)\n",
    "print(x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 算术运算\n",
    "print(np.add(x1,x2))\n",
    "print(np.subtract(x1,x2))\n",
    "print(np.multiply(x1,x2))\n",
    "print(np.divide(x1,x2))\n",
    "print(np.power(x1,x2))\n",
    "print(np.remainder(x1,x2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 统计运算\n",
    "import numpy as np\n",
    "a = np.array([[1,2,3],[4,5,6],[7,8,9]])\n",
    "\n",
    "# 统计矩阵最小值\n",
    "print(np.amin(a))\n",
    "print(np.amin(a,0))\n",
    "print(np.amin(a,1))\n",
    "\n",
    "# 统计矩阵最大值\n",
    "print(np.amax(a))\n",
    "print(np.amax(a,0))\n",
    "print(np.amax(a,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 统计最大值和最小值之差\n",
    "print(np.ptp(a))\n",
    "print(np.ptp(a,0))\n",
    "print(np.ptp(a,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 统计数组的百分位数 percentile()\n",
    "print(np.percentile(a,50))\n",
    "print(np.percentile(a,50,axis=0))\n",
    "print(np.percentile(a,50,axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 统计数组中的中位数 median()\n",
    "print(np.median(a))\n",
    "print(np.median(a,axis=0))\n",
    "print(np.median(a,axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 统计数组中的平均数 mean()\n",
    "print(np.mean(a))\n",
    "print(np.mean(a,axis=0))\n",
    "print(np.mean(a,axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加权平均值 average()\n",
    "source=np.array([1,2,3,4])\n",
    "wts=np.array([1,2,3,4])\n",
    "print(np.average(a))\n",
    "# 加权\n",
    "print(np.average(source,weights=wts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 统计数组中的标准差 std() 、方差 var()\n",
    "source=np.array([1,2,3,4])\n",
    "print(np.std(source))\n",
    "print(np.var(source))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numpy 排序\n",
    "source=np.array([[4,2,3,1],[2,4,1,7],[3,2,6,0]])\n",
    "print(np.sort(source))\n",
    "print(np.sort(source,axis=None))\n",
    "print(np.sort(source,axis=0))\n",
    "print(np.sort(source,axis=0))\n",
    "print(np.sort(source,axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 实例\n",
    "import numpy as np\n",
    "\n",
    "def show(name,data):\n",
    "    print('%s，平均成绩：%d,最小成绩：%d,最大成绩：%d,方差：%d,标准差：%d' % (name,np.average(data),np.amin(data),np.amax(data),np.var(data),np.std(data)))\n",
    "\n",
    "score_type=np.dtype({\n",
    "    'names':['name','chinese','english','math'],\n",
    "    'formats':['S32','i','i','i']\n",
    "})\n",
    "\n",
    "students=np.array([\n",
    "    ('zhangfei',66,65,30),\n",
    "    ('guanyu',95,85,98),\n",
    "    ('zhaoyun',93,92,96),\n",
    "    ('huangzhong',90,88,77),\n",
    "    ('dianwei',80,90,90)\n",
    "],dtype=score_type)\n",
    "\n",
    "show('chinese',students[:]['chinese'])\n",
    "show('english',students[:]['english'])\n",
    "show('math',students[:]['math'])\n",
    "\n",
    "rank=sorted(students,key=lambda x:x[1]+x[2]+x[3],reverse=True)\n",
    "print(rank)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 05 Python科学计算：Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import Series,DataFrame\n",
    "\n",
    "x1=Series([1,2,3,4,5,6,7,8,9])\n",
    "x2=Series(data=[1,2,3,4,5],index=['a','b','c','d','e'])\n",
    "\n",
    "data={'a':1,'b':2,'c':3,'d':4,'e':5}\n",
    "x3=Series(data)\n",
    "\n",
    "print(x1)\n",
    "print(x2)\n",
    "print(x3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import Series, DataFrame\n",
    "import numpy as np\n",
    "\n",
    "data = {'Chinese': [66, 95, 93, 90,80],'English': [65, 85, 92, 88, 90],'Math': [30, 98, 96, 77, 90]}\n",
    "df1= DataFrame(data)\n",
    "df2 = DataFrame(data, index=['ZhangFei', 'GuanYu', 'ZhaoYun', 'HuangZhong', 'DianWei'], columns=['English', 'Math', 'Chinese'])\n",
    "\n",
    "print(df1)\n",
    "print(df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 清洗数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 删除 DataFrame 中的不必要的列和行\n",
    "del_columns=df2.drop(columns=['Chinese'])\n",
    "print(del_columns)\n",
    "\n",
    "del_line=df2.drop(index=['ZhangFei'])\n",
    "print(del_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 重命名列名 columns\n",
    "df2.rename(columns={'Chinese':'YunWen','English':'YinYu'},inplace=True)\n",
    "print(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 去除重复行\n",
    "remove_duplicate=df2.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['YunWen'].astype(np.int64)\n",
    "df2['YunWen']= df2['YunWen'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 删除左右两边空格\n",
    "df2['YunWen']=df2['YunWen'].map(str.strip)\n",
    "# 删除左边空格\n",
    "df2['YunWen']=df2['YunWen'].map(str.lstrip)\n",
    "# 删除右边空格\n",
    "df2['YunWen']=df2['YunWen'].map(str.rstrip)\n",
    "\n",
    "# 去除特殊字符\n",
    "df2['YunWen']=df2['YunWen'].str.strip('$')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 全部大写\n",
    "df2.columns=df2.columns.str.upper()\n",
    "# 全部小写\n",
    "df2.columns=df2.columns.str.lower()\n",
    "# 首字母大写\n",
    "df2.columns=df2.columns.str.title()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 统计函数合集\n",
    "df3 = DataFrame({'name':['ZhangFei', 'GuanYu', 'a', 'b', 'c'], 'data1':range(5)})\n",
    "print(df3.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据表合并\n",
    "df4 = DataFrame({'name':['ZhangFei', 'GuanYu', 'a', 'b', 'c'], 'data1':range(5)})\n",
    "df5 = DataFrame({'name':['ZhangFei', 'GuanYu', 'A', 'B', 'C'], 'data2':range(5)})\n",
    "\n",
    "# 内连接（交集）\n",
    "df6 = pd.merge(df4,df5,how='inner')\n",
    "# 左连接\n",
    "df6 = pd.merge(df4,df5,how='left')\n",
    "# 右连接\n",
    "df6 = pd.merge(df4,df5,how='right')\n",
    "# 外连接（并集）\n",
    "df6 = pd.merge(df4,df5,how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用 SQL\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "from pandasql import sqldf, load_meat, load_births\n",
    "df7 = DataFrame({'name':['ZhangFei', 'GuanYu', 'a', 'b', 'c'], 'data1':range(5)})\n",
    "pysqldf = lambda sql: sqldf(sql, globals())\n",
    "sql = \"select * from df7 where name ='ZhangFei'\"\n",
    "print(pysqldf(sql))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 例子\n",
    "import pandas as pd\n",
    "from pandas import Series, DataFrame\n",
    "import numpy as np\n",
    "\n",
    "data = {'Chinese': [66, 95, 95, 90,80,80],'English': [65, 85, 92, 88, 90,90],'Math': [None, 98, 96, 77, 90,90]}\n",
    "df = DataFrame(data, index=['ZhangFei', 'GuanYu', 'ZhaoYun', 'HuangZhong', 'DianWei','DianWei'], columns=['Chinese','English', 'Math' ])\n",
    "\n",
    "print(df)\n",
    "\n",
    "df=df.drop_duplicates()\n",
    "df['Total']=df.sum(axis=1)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 13 数据变换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Min-Max 规范化\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "\n",
    "x = np.array([\n",
    "    [0.,-3.,1.],\n",
    "    [3.,1.,2.],\n",
    "    [0.,1.,-1.]\n",
    "])\n",
    "\n",
    "min_max_scaler=preprocessing.MinMaxScaler()\n",
    "minmax_x=min_max_scaler.fit_transform(x)\n",
    "\n",
    "print(minmax_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Z-Score 规范化\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "\n",
    "x = np.array([\n",
    "    [0.,-3.,1.],\n",
    "    [3.,1.,2.],\n",
    "    [0.,1.,-1.]\n",
    "])\n",
    "\n",
    "scaled_x=preprocessing.scale(x)\n",
    "print(scaled_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. 小数定标规范化\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "\n",
    "x = np.array([\n",
    "    [0.,-3.,1.],\n",
    "    [3.,1.,2.],\n",
    "    [0.,1.,-1.]\n",
    "])\n",
    "\n",
    "j=np.ceil(np.log10(np.max(abs(x))))\n",
    "scaled_x=x/(10**j)\n",
    "print(scaled_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 习题\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "\n",
    "x = np.array([\n",
    "    [5000.],\n",
    "    [16000.],\n",
    "    [58000.]\n",
    "])\n",
    "\n",
    "min_max_scaler=preprocessing.MinMaxScaler()\n",
    "minmax_x=min_max_scaler.fit_transform(x)\n",
    "\n",
    "print(minmax_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 15 数据可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 散点图\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "N=1000\n",
    "x=np.random.rand(N)\n",
    "y=np.random.rand(N)\n",
    "\n",
    "plt.scatter(x,y,marker='x')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'x':x,'y':y})\n",
    "sns.jointplot(x='x',y='y',data=df,kind='scatter')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 折线图\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "x = [2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019]\n",
    "y = [5, 3, 6, 20, 17, 16, 19, 30, 32, 35]\n",
    "\n",
    "plt.plot(x,y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame({'x':x,'y':y})\n",
    "sns.lineplot(x='x',y='y',data=df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 直方图\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "a=np.random.rand(100)\n",
    "s=pd.Series(a)\n",
    "\n",
    "plt.hist(s)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(s,kde=False)\n",
    "plt.show()\n",
    "sns.distplot(s,kde=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 条形图\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "x=['Cat1','Cat2','Cat3','Cat4','Cat5']\n",
    "y=[5,4,8,12,7]\n",
    "\n",
    "plt.bar(x,y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x,y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 箱线图\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "data=np.random.normal(size=(10,4))\n",
    "labels=['A','B','C','D']\n",
    "\n",
    "plt.boxplot(data,labels=labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame(data,columns=labels)\n",
    "sns.boxplot(data=df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 饼图\n",
    "import matplotlib.pyplot as plt\n",
    "# 数据准备\n",
    "nums = [25, 37, 33, 37, 6]\n",
    "labels = ['High-school','Bachelor','Master','Ph.d', 'Others']\n",
    "# 用Matplotlib画饼图\n",
    "plt.pie(x = nums, labels=labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 热力图\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "# 数据准备\n",
    "flights = DataFrame(pd.read_csv('~/Project/seaborn-data/flights.csv'))\n",
    "data=flights.pivot('year','month','passengers')\n",
    "# 用Seaborn画热力图\n",
    "sns.heatmap(data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 蜘蛛图\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.font_manager import FontProperties\n",
    "\n",
    "labels=np.array([u\"推进\",\"KDA\",u\"生存\",u\"团战\",u\"发育\",u\"输出\"])\n",
    "stats=[83, 61, 95, 67, 76, 88]\n",
    "\n",
    "angles=np.linspace(0,2*np.pi,len(labels),endpoint=False)\n",
    "stats=np.concatenate((stats,[stats[0]]))\n",
    "angles=np.concatenate((angles,[angles[0]]))\n",
    "\n",
    "fig=plt.figure()\n",
    "ax=fig.add_subplot(111,polar=True)\n",
    "ax.plot(angles, stats, 'o-', linewidth=2)\n",
    "ax.fill(angles, stats, alpha=0.25)\n",
    "# 设置中文字体\n",
    "font = FontProperties(fname=r\"C:\\Windows\\Fonts\\simhei.ttf\", size=14)  \n",
    "ax.set_thetagrids(angles * 180/np.pi, labels, FontProperties=font)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 二元变量分布\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "# 数据准备\n",
    "tips = DataFrame(pd.read_csv('~/Project/seaborn-data/tips.csv'))\n",
    "# print(tips.head(10))\n",
    "# 用Seaborn画二元变量分布图（散点图，核密度图，Hexbin图）\n",
    "sns.jointplot(x=\"total_bill\", y=\"tip\", data=tips, kind='scatter')\n",
    "sns.jointplot(x=\"total_bill\", y=\"tip\", data=tips, kind='kde')\n",
    "sns.jointplot(x=\"total_bill\", y=\"tip\", data=tips, kind='hex')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 成对关系\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "# 数据准备\n",
    "iris = DataFrame(pd.read_csv('~/Project/seaborn-data/iris.csv'))\n",
    "# 用Seaborn画成对关系\n",
    "sns.pairplot(iris)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 18 决策树"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 使用 CART 算法来创建分类树"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.datasets import load_iris\n",
    "from pandas import DataFrame\n",
    "import graphviz\n",
    "\n",
    "# 准备数据集\n",
    "iris = load_iris()\n",
    "# 获取特征集和分类标识\n",
    "features = iris.data\n",
    "labels = iris.target\n",
    "\n",
    "# 随机抽取 33% 的数据作为测试集，其余为训练集\n",
    "train_features,test_features,train_labels,test_labels = train_test_split(features,labels,test_size=0.33,random_state=0)\n",
    "\n",
    "# 创建 CART 分类树\n",
    "clf = DecisionTreeClassifier(criterion='gini')\n",
    "# 拟合构造 CART 分类树\n",
    "clf = clf.fit(train_features,train_labels)\n",
    "# 用 CART 分类树做预测\n",
    "test_predict = clf.predict(test_features)\n",
    "score = accuracy_score(test_labels,test_predict)\n",
    "print(\"CART 分类树准确率 %.4lf\" % score)\n",
    "# 决策树\n",
    "# dot_data = export_graphviz(clf,out_file=None)\n",
    "# graph = graphviz.Source(dot_data)\n",
    "# graph.render('iris')"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 使用 CART 回归树做预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.metrics import r2_score,mean_absolute_error,mean_squared_error\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# 准备数据集\n",
    "boston = load_boston()\n",
    "# 探索数据\n",
    "print(boston.feature_names)\n",
    "# 获取特征集和房价\n",
    "features = boston.data\n",
    "prices = boston.target\n",
    "# 随机抽取33%的数据作为测试集，其余为训练集\n",
    "train_features,test_features,train_price,test_price = train_test_split(features,prices,test_size=0.33)\n",
    "# 创建CART回归树\n",
    "dtr = DecisionTreeRegressor()\n",
    "# 拟合构造CART回归树\n",
    "dtr.fit(train_features,train_price)\n",
    "# 预测测试集中的房价\n",
    "predict_price = dtr.predict(test_features)\n",
    "# 测试集的结果评价\n",
    "print('回归树二乘偏差均值',mean_squared_error(test_price,predict_price))\n",
    "print('回归树绝对值偏差均值',mean_absolute_error(test_price,predict_price))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 泰坦尼克号生存预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# 数据加载\n",
    "train_data = pd.read_csv('~/Project/Titanic_Data/train.csv')\n",
    "test_data = pd.read_csv('~/Project/Titanic_Data/test.csv')\n",
    "# print(train_data.info())\n",
    "# print(train_data.describe())\n",
    "\n",
    "# 使用平均年龄来填充年龄中的 nan 值\n",
    "train_data['Age'].fillna(train_data['Age'].mean(),inplace=True)\n",
    "test_data['Age'].fillna(test_data['Age'].mean(),inplace=True)\n",
    "# 使用票价的均值填充票价中的 nan 值\n",
    "train_data['Fare'].fillna(train_data['Fare'].mean(),inplace=True)\n",
    "test_data['Fare'].fillna(test_data['Fare'].mean(),inplace=True)\n",
    "\n",
    "# print(train_data['Embarked'].value_counts())\n",
    "\n",
    "# 使用登陆最多的港口来填充登陆港口的 nan 值\n",
    "train_data['Embarked'].fillna('S',inplace=True)\n",
    "test_data['Embarked'].fillna('S',inplace=True)\n",
    "\n",
    "# 特征选择\n",
    "features = ['Pclass','Sex','Age','SibSp','Parch','Fare','Embarked']\n",
    "train_features = train_data[features]\n",
    "train_labels = train_data['Survived']\n",
    "test_features = test_data[features]\n",
    "\n",
    "dvec = DictVectorizer(sparse=False)\n",
    "train_features = dvec.fit_transform(train_features.to_dict(orient='record'))\n",
    "print(dvec.feature_names_)\n",
    "\n",
    "# 决策树模型\n",
    "# 构造 ID3 决策树\n",
    "clf = DecisionTreeClassifier(criterion='entropy')\n",
    "# 决策树训练\n",
    "clf.fit(train_features,train_labels)\n",
    "\n",
    "test_features = dvec.transform(test_features.to_dict(orient='record'))\n",
    "# 决策树预测\n",
    "pred_labels=clf.predict(test_features)\n",
    "\n",
    "# 得到决策树的准确率\n",
    "acc_decision_tree = round(clf.score(train_features,train_labels),6)\n",
    "print(u'score 准确率 %.4lf' % acc_decision_tree)\n",
    "\n",
    "# 使用 k 折交叉验证 统计决策树准确率\n",
    "print(u'cross_val_score 准确率 %.4lf' % np.mean(cross_val_score(clf,train_features,train_labels,cv=10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 朴素贝叶斯分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vec = TfidfVectorizer()\n",
    "\n",
    "documents = [\n",
    "    'this is the bayes document',\n",
    "    'this is the second second document',\n",
    "    'and the third one',\n",
    "    'is this the document'\n",
    "]\n",
    "tfidf_matrix = tfidf_vec.fit_transform(documents)\n",
    "\n",
    "print('不重复的词:',tfidf_vec.get_feature_names())\n",
    "print('每个单词的ID:',tfidf_vec.vocabulary_)\n",
    "print('每个单词的 tfidf 值:',tfidf_matrix.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 文档分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "import os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "#  加载停用词列表\n",
    "def load_stop_wrods():\n",
    "    stop_word=''\n",
    "    with open('text_classification/text classification/stop/stopword.txt','r', encoding='utf-8') as f:\n",
    "        stop_word=f.read();\n",
    "        stop_word= stop_word.encode('utf-8').decode('utf-8-sig')\n",
    "        stop_word=stop_word.split('\\n')\n",
    "    return stop_word\n",
    "\n",
    "# 加载文件\n",
    "def load_document(file_dir,label):\n",
    "    file_list = os.listdir(file_dir)\n",
    "    words_list = []\n",
    "    label_list = []\n",
    "    for file in file_list:\n",
    "        file_path=os.path.join(file_dir,file)\n",
    "        words_list.append(cut_text(file_path))\n",
    "        label_list.append(label)\n",
    "    return words_list,label_list\n",
    "\n",
    "# 分词\n",
    "def cut_text(file_path):\n",
    "    word_spilit=''\n",
    "    result = ''\n",
    "    with open(file_path,'r', encoding='gb18030') as f:\n",
    "        text=f.read()\n",
    "        cut_text = jieba.cut(text)\n",
    "        result=word_spilit.join(cut_text)\n",
    "    return result\n",
    "\n",
    "\n",
    "# 训练数据\n",
    "train_words_list1, train_labels1 = load_document('text_classification/text classification/train/女性', '女性')\n",
    "train_words_list2, train_labels2 = load_document('text_classification/text classification/train/体育', '体育')\n",
    "train_words_list3, train_labels3 = load_document('text_classification/text classification/train/文学', '文学')\n",
    "train_words_list4, train_labels4 = load_document('text_classification/text classification/train/校园', '校园')\n",
    "\n",
    "train_words_list = train_words_list1 + train_words_list2 + train_words_list3 + train_words_list4\n",
    "train_labels = train_labels1 + train_labels2 + train_labels3 + train_labels4\n",
    "\n",
    "# 测试数据\n",
    "test_words_list1, test_labels1 = load_document('text_classification/text classification/test/女性', '女性')\n",
    "test_words_list2, test_labels2 = load_document('text_classification/text classification/test/体育', '体育')\n",
    "test_words_list3, test_labels3 = load_document('text_classification/text classification/test/文学', '文学')\n",
    "test_words_list4, test_labels4 = load_document('text_classification/text classification/test/校园', '校园')\n",
    "\n",
    "test_words_list = test_words_list1 + test_words_list2 + test_words_list3 + test_words_list4\n",
    "test_labels = test_labels1 + test_labels2 + test_labels3 + test_labels4\n",
    "\n",
    "stop_words=load_stop_wrods()\n",
    "\n",
    "# 计算权重\n",
    "train_tf=TfidfVectorizer(stop_words=stop_words,max_df=0.5)\n",
    "train_features=train_tf.fit_transform(train_words_list)\n",
    "\n",
    "# 多项式贝叶斯分类器\n",
    "clf=MultinomialNB(alpha=0.01).fit(train_features,train_labels)\n",
    "\n",
    "# 使用生成分类器做预测\n",
    "test_tf=TfidfVectorizer(stop_words=stop_words,max_df=0.5,vocabulary=tf.vocabulary_)\n",
    "test_features=test_tf.fit_transform(test_words_list)\n",
    "\n",
    "predicted_labels=clf.predict(test_features)\n",
    "\n",
    "# 计算准确率\n",
    "print('准确率为：', metrics.accuracy_score(test_labels, predicted_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}